import os
import sys
import chardet
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from typing import Dict, Any
import openai
from tenacity import retry, stop_after_attempt, wait_exponential
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.impute import SimpleImputer
from dotenv import load_dotenv
import requests

# Load environment variables
load_dotenv('file_name.env')

# Proxy URL for OpenAI API through AI Proxy
proxy_url = "https://aiproxy.sanand.workers.dev"
try:
    response = requests.get(proxy_url)
    print(f"Proxy is reachable: {response.status_code}")
except Exception as e:
    print(f"Error reaching proxy: {e}")

class AutomatedAnalysis:
    def __init__(self, dataset_path: str):
        """
        Initialize the analysis with the given dataset

        Args:
            dataset_path (str): Path to the input CSV file
        """
        self.dataset_path = dataset_path
        self.encoding = self.detect_encoding()
        self.df = self.load_dataset()
        self.preprocess_data()
        self.api_token = self.get_api_token()
        openai.api_base = "https://aiproxy.sanand.workers.dev/openai/v1"
        openai.api_key = self.api_token

    def detect_encoding(self) -> str:
        """
        Detect the encoding of the input file

        Returns:
            str: Detected encoding
        """
        with open(self.dataset_path, 'rb') as file:
            raw_data = file.read()
            result = chardet.detect(raw_data)
        return result['encoding'] or 'utf-8'

    def load_dataset(self) -> pd.DataFrame:
        """
        Load the dataset with the detected encoding

        Returns:
            pd.DataFrame: Loaded dataset
        """
        try:
            df = pd.read_csv(self.dataset_path, encoding=self.encoding, low_memory=False, on_bad_lines='skip')
            print(f"Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns.")
            return df
        except Exception as e:
            print(f"Error loading dataset: {e}")
            raise ValueError(f"Error loading dataset: {e}")

    def preprocess_data(self):
        """
        Preprocess the dataset by handling missing values, data types, and special columns
        """
        # Handle date columns
        if 'date' in self.df.columns:
            self.df['date'] = pd.to_datetime(self.df['date'], errors='coerce')  # Convert to datetime
        
        # Handle categorical columns (e.g., 'language', 'type')
        categorical_cols = self.df.select_dtypes(include=['object']).columns
        self.df[categorical_cols] = self.df[categorical_cols].fillna('Unknown')  # Replace NaNs with 'Unknown'
        
        # Handle missing values for numeric columns
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        imputer = SimpleImputer(strategy='median')
        self.df[numeric_cols] = imputer.fit_transform(self.df[numeric_cols])

    def get_api_token(self) -> str:
        """
        Retrieve API token from environment variable

        Returns:
            str: API Token
        """
        token = os.getenv("AIPROXY_TOKEN")
        if not token:
            raise EnvironmentError("AIPROXY_TOKEN environment variable is not set.")
        return token

    def get_data_summary(self) -> Dict[str, Any]:
        """
        Generate a summary of the dataset

        Returns:
            Dict[str, Any]: Dataset summary
        """
        return {
            "total_rows": len(self.df),
            "total_columns": len(self.df.columns),
            "column_types": self.df.dtypes.apply(str).to_dict(),
            "missing_values": self.df.isnull().sum().to_dict(),
            "numeric_summary": self.df.describe().to_dict()
        }

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def generate_narrative(self, summary: Dict[str, Any], analysis_results: Dict[str, Any]) -> str:
        """
        Generate a narrative about the dataset using the LLM

        Args:
            summary (Dict[str, Any]): Dataset summary
            analysis_results (Dict[str, Any]): Results of the analysis

        Returns:
            str: Narrative generated by the LLM
        """
        prompt = (
            f"Dataset Analysis:\n\n"
            f"Summary:\n{summary}\n\n"
            f"Analysis Results:\n{analysis_results}\n\n"
            "Write a detailed, well-structured Markdown summary of the dataset analysis. "
            "Include an overview of the data, key findings, and potential implications."
        )

        try:
            response = openai.ChatCompletion.create(
                model="gpt-4o-mini",  # Proxy-supported model
                messages=[{
                    "role": "system", "content": "You are an expert data analyst."
                }, {
                    "role": "user", "content": prompt
                }],
                max_tokens=500
            )

            # Extracting the generated narrative from the response
            narrative = response['choices'][0]['message']['content'].strip()

            return narrative

        except Exception as e:
            # Log the error and provide a fallback message
            print(f"Error generating narrative: {e}")
            return "An error occurred while generating the narrative."

    def save_results(self, summary: Dict[str, Any], analysis_results: Dict[str, Any], narrative: str):
        """
        Save the analysis results and narrative to files

        Args:
            summary (Dict[str, Any]): Dataset summary
            analysis_results (Dict[str, Any]): Analysis results
            narrative (str): Generated narrative
        """
        folder_name = self.dataset_path.split('.')[0]
        os.makedirs(folder_name, exist_ok=True)

        # Save narrative to README.md
        try:
            with open(f"{folder_name}/README.md", "w") as f:
                f.write("# Automated Dataset Analysis\n\n")
                f.write(narrative)
                f.write("\n\n## Summary of Dataset\n")
                f.write(f"```\n{summary}\n```\n\n")
                f.write("## Clustering Analysis Results\n")
                f.write(f"```\n{analysis_results}\n```\n")
        except Exception as e:
            print(f"Error saving README.md: {e}")

        # Save visualizations
        self.save_visualizations(folder_name)

    def perform_analysis(self):
        """
        Perform analysis on the dataset and generate outputs
        """
        summary = self.get_data_summary()
        analysis_results = self.perform_clustering()
        narrative = self.generate_narrative(summary, analysis_results)
        self.save_results(summary, analysis_results, narrative)

    def perform_clustering(self) -> Dict[str, Any]:
        """
        Perform clustering analysis on numeric columns

        Returns:
            Dict[str, Any]: Clustering results
        """
        numeric_data = self.df.select_dtypes(include=[np.number])  # Only numeric columns
        if numeric_data.empty:
            raise ValueError("No numeric columns found for clustering.")

        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(numeric_data)

        kmeans = KMeans(n_clusters=3, random_state=42)
        self.df['Cluster'] = kmeans.fit_predict(scaled_data)
        
        return {
            "inertia": kmeans.inertia_,
            "cluster_centers": kmeans.cluster_centers_.tolist()
        }

    def save_visualizations(self, folder_name: str):
        """
        Save data visualizations as PNG files
        """
        numeric_data = self.df.select_dtypes(include=[np.number])

        # Correlation heatmap
        plt.figure(figsize=(10, 8))
        sns.heatmap(numeric_data.corr(), annot=True, cmap="coolwarm")
        plt.title("Correlation Heatmap")
        plt.savefig(f"{folder_name}/correlation_heatmap.png")

        # Cluster distribution
        plt.figure(figsize=(8, 6))
        sns.countplot(x='Cluster', data=self.df)
        plt.title("Cluster Distribution")
        plt.savefig(f"{folder_name}/cluster_distribution.png")


if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python autolysis.py <dataset.csv>")
        sys.exit(1)

    dataset_path = sys.argv[1]
    analysis = AutomatedAnalysis(dataset_path)
    analysis.perform_analysis()
